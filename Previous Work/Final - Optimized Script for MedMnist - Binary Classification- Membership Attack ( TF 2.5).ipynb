{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca10298",
   "metadata": {},
   "source": [
    "# Importing Libraries "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e0f81a9",
   "metadata": {},
   "source": [
    "#### To ensure compatibility with the MIA library, we are utilizing Tensorflow 2.5 and numpy 1.19 since the MIA library is not compatible with higher versions of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import copy\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "\n",
    " \n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218586c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow_privacy.privacy.optimizers import dp_optimizer_keras\n",
    "import tensorflow_privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c58a5ca2",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from 'X_train.npy'\n",
    "X_train = np.load('X_train.npy')\n",
    "# Load testing data from 'X_test.npy'\n",
    "X_test = np.load('X_test.npy')\n",
    "\n",
    "# Load training labels from 'y_train.npy'\n",
    "y_train = np.load('y_train.npy')\n",
    "# Load testing labels from 'y_test.npy'\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape) # Print the shape of X_train array\n",
    "print(\"Shape of X_test:\", X_test.shape) # Print the shape of X_test array\n",
    "print(\"Shape of y_train:\", y_train.shape) # Print the shape of y_train array\n",
    "print(\"Shape of y_test:\", y_test.shape) # Print the shape of y_test array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de004554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the membership inference attack accepts data in multiples of the batch size, \n",
    "#we will access the first 5000 elements for X_train and y_train\n",
    "\n",
    "X_train = X_train[0:5000]\n",
    "y_train = y_train[0:5000]\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afcaee29",
   "metadata": {},
   "source": [
    "### Using keras data loader to load numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ff216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator()\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=10)\n",
    "test_generator = datagen.flow(X_test, y_test, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20255374",
   "metadata": {},
   "source": [
    "### Non Differentially Private Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn_keras():\n",
    "    \"\"\"\n",
    "    Creates and compiles a target model based on the DenseNet121 architecture.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled target model\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a new sequential model\n",
    "    model = Sequential() \n",
    "\n",
    "    # Load the pre-trained DenseNet121 model with weights from 'DenseNet-BC-121-32-no-top.h5'\n",
    "    base_model = DenseNet121(weights='DenseNet-BC-121-32-no-top.h5', include_top=False) \n",
    "\n",
    "    # Add the DenseNet121 model to the new sequential model\n",
    "    model.add(base_model)\n",
    "\n",
    "    # Add a global average pooling layer\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Add a dense output layer with the appropriate number of classes (2 classes in this case)\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Define the loss function as CategoricalCrossentropy with logits and no reduction\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "    # Compile the model with Adam optimizer, the defined loss function, and accuracy as the evaluation metric\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    # Return the compiled model\n",
    "    return model\n",
    "\n",
    "\n",
    "def attack_model_fn_keras():\n",
    "    \"\"\"\n",
    "    Creates and compiles an attack model based on a dense architecture.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled attack model\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a new sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add a dense layer with 128 units and ReLU activation function\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(2,)))\n",
    "\n",
    "    # Apply dropout regularization with a rate of 0.3\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "\n",
    "    # Add a dense layer with 64 units and ReLU activation function\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    # Apply dropout regularization with a rate of 0.2\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "\n",
    "    # Add another dense layer with 64 units and ReLU activation function\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    # Add a dense output layer with 1 unit and sigmoid activation function\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model with Adam optimizer, binary cross-entropy loss function, and accuracy metric\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Return the compiled model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an EarlyStopping callback to stop training if the loss does not improve for 3 consecutive epochs\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "## Print a message indicating that the target model training is starting\n",
    "print(\"Training the target model with keras model\")\n",
    "\n",
    "## Create the target model using the target_model_fn_keras() function\n",
    "target_model = target_model_fn_keras()\n",
    "\n",
    "## Fit the target model using the training generator\n",
    "## Train for 18 epochs, display verbose output, and use the EarlyStopping callback\n",
    "## Store the training history in the history_target_model variable\n",
    "history_target_model = target_model.fit_generator(\n",
    "    train_generator, epochs=18, verbose=True, callbacks=[callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdf136",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict the labels for the test set using the target model\n",
    "y_test_hat = target_model.predict(X_test, batch_size=4)\n",
    "\n",
    "## Convert the predicted probabilities to class labels by selecting the index with the highest probability\n",
    "y_test_hat = np.argmax(y_test_hat, axis=1)\n",
    "\n",
    "## Convert the original labels from one-hot encoding to class labels\n",
    "y_test_org = np.argmax(y_test, axis=1)\n",
    "\n",
    "## Calculate the confusion matrix and classification report\n",
    "conf_m = confusion_matrix(y_test_org, y_test_hat)\n",
    "clas_r = classification_report(y_test_org, y_test_hat)\n",
    "\n",
    "## Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(conf_m, annot=True, xticklabels=['H', 'P'], yticklabels=['H', 'P'], cbar=False, cmap='Blues', linewidths=1, linecolor='black', fmt='.0f')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "ax.xaxis.set_ticks_position('top') \n",
    "plt.title('Confusion matrix - test data\\n(H - healthy/normal, P - pneumonia)')\n",
    "plt.show()\n",
    "\n",
    "## Print the classification report on the test data for the Non DP Model\n",
    "print('Classification report on test data for Non DP Model')\n",
    "print(clas_r)\n",
    "\n",
    "## Calculate the accuracy score\n",
    "acc_score = accuracy_score(y_test_org, y_test_hat)\n",
    "print(acc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a ShadowModelBundle with the specified target model function, shadow dataset size, and number of models\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn_keras,\n",
    "    shadow_dataset_size=200,\n",
    "    num_models=5,\n",
    ")\n",
    "\n",
    "## Split the attacker dataset into training and test sets\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(X_test, y_test, test_size=0.1)\n",
    "\n",
    "## Print the shapes of the attacker training and test sets\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "## Take the first 500 samples from the attacker training set\n",
    "attacker_X_train = attacker_X_train[0:500]\n",
    "attacker_y_train = attacker_y_train[0:500]\n",
    "\n",
    "## Take the first 60 samples from the attacker test set\n",
    "attacker_X_test = attacker_X_test[0:60]\n",
    "attacker_y_test = attacker_y_test[0:60]\n",
    "\n",
    "## Fit the ShadowModelBundle on the attacker training data and transform it\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=10,\n",
    "        verbose=False,\n",
    "        validation_data=(attacker_X_test, attacker_y_test),\n",
    "        batch_size=10\n",
    "    ),\n",
    ")\n",
    "\n",
    "## Create an AttackModelBundle with the specified attack model function and number of classes\n",
    "amb = AttackModelBundle(attack_model_fn_keras, num_classes=2)\n",
    "\n",
    "## Fit the attack models using the shadow data\n",
    "amb.fit(\n",
    "    X_shadow, y_shadow, fit_kwargs=dict(epochs=10, verbose=False)\n",
    ")\n",
    "\n",
    "## Define the size of the attack test dataset\n",
    "ATTACK_TEST_DATASET_SIZE = 200\n",
    "\n",
    "## Prepare the attack data in the expected format for the AttackModelBundle\n",
    "data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
    "data_out = X_test[:ATTACK_TEST_DATASET_SIZE], y_test[:ATTACK_TEST_DATASET_SIZE]\n",
    "\n",
    "## Compile the attack test data and real membership labels\n",
    "attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "    target_model, data_in, data_out\n",
    ")\n",
    "\n",
    "## Predict the membership labels using the AttackModelBundle\n",
    "attack_guesses = amb.predict(attack_test_data)\n",
    "\n",
    "## Compute the attack accuracy by comparing the predicted labels with the real membership labels\n",
    "attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5af5d647",
   "metadata": {},
   "source": [
    "### Differentially Private Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mia_attack(noise_multiplier,l2_norm_clip ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform MIA attack.\n",
    "\n",
    "    Args:\n",
    "    - noise_multiplier: Noise multiplier for DP optimizer\n",
    "    - l2_norm_clip: L2 norm clip for DP optimizer\n",
    "\n",
    "    Returns:\n",
    "    - attack_accuracy: Attack accuracy\n",
    "    - epsilon: Privacy parameter epsilon\n",
    "    - acc_score: Accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def target_model_fn_keras_dp():\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates and compiles a target model with Differential Privacy (DP) optimizer.\n",
    "\n",
    "        Returns:\n",
    "        - model: Compiled target model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## Load the pre-trained DenseNet121 model with weights\n",
    "        base_model = DenseNet121(weights='DenseNet-BC-121-32-no-top.h5', include_top=False, input_shape = (224, 224, 3))\n",
    "\n",
    "        # Create a new sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add the DenseNet121 model to the new sequential model\n",
    "        model.add(base_model)\n",
    "\n",
    "        # Add a global average pooling layer\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "\n",
    "        # Add a dense output layer with the appropriate number of classes\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        # Define the loss function as CategoricalCrossentropy with logits and no reduction\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    \n",
    "\n",
    "        # Create a DP optimizer with specified parameters\n",
    "        optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        num_microbatches=10,\n",
    "        learning_rate=0.25)\n",
    "\n",
    "        \n",
    "        # Compile the model with the DP optimizer, loss function, and accuracy metric\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "     \n",
    "\n",
    "    def attack_model_fn_keras_dp():\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates and compiles an attack model with Differential Privacy (DP) optimizer.\n",
    "\n",
    "        Returns:\n",
    "        - model: Compiled attack model\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a new sequential model\n",
    "        model = tf.keras.models.Sequential()\n",
    "        \n",
    "        # Add a dense layer with 128 units and ReLU activation function\n",
    "        model.add(layers.Dense(128, activation=\"relu\", input_shape=(2,)))\n",
    "\n",
    "        # Apply dropout regularization with a rate of 0.3\n",
    "        model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "        \n",
    "        # Add a dense layer with 64 units and ReLU activation function\n",
    "        model.add(layers.Dense(64, activation=\"relu\"))\n",
    "        \n",
    "        # Apply dropout regularization with a rate of 0.2\n",
    "        model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "        \n",
    "        # Add another dense layer with 64 units and ReLU activation function\n",
    "        model.add(layers.Dense(64, activation=\"relu\"))\n",
    "        \n",
    "        # Add a dense output layer with 1 unit and sigmoid activation function\n",
    "        model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "        \n",
    "        # Compile the model with Adam optimizer, binary cross-entropy loss function, and accuracy metric\n",
    "        model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "        return model  \n",
    "    \n",
    "    \n",
    "    ## Create the target model using the target_model_fn_keras() function\n",
    "    target_model = target_model_fn_keras_dp()\n",
    "    \n",
    "    ## Create an EarlyStopping callback to stop training if the loss does not improve for 3 consecutive epochs\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    \n",
    "\n",
    "    ## Fit the target model using the training generator\n",
    "    ## Train for 18 epochs, display verbose output, and use the EarlyStopping callback\n",
    "    ## Store the training history in the history_target_model variable\n",
    "    history_target_model = target_model.fit_generator(\n",
    "            train_generator, epochs=18, verbose=True, callbacks=[callback]\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Predict the labels for the test set using the target model\n",
    "    y_test_hat = target_model.predict(X_test, batch_size=4)\n",
    "    \n",
    "    ## Convert the predicted probabilities to class labels by selecting the index with the highest probability\n",
    "    y_test_hat = np.argmax(y_test_hat, axis=1)\n",
    "    \n",
    "    ## Convert the original labels from one-hot encoding to class labels\n",
    "    y_test_org = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    ## Calculate the accuracy score\n",
    "    acc_score = accuracy_score(y_test_org, y_test_hat)\n",
    "    \n",
    "\n",
    "    ## Create a ShadowModelBundle with the specified target model function, shadow dataset size, and number of models\n",
    "    smb = ShadowModelBundle(\n",
    "        target_model_fn_keras_dp,\n",
    "        shadow_dataset_size=200,\n",
    "        num_models=5,\n",
    "    )\n",
    "    \n",
    "    ## Split the attacker dataset into training and test sets\n",
    "    attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(X_test, y_test, test_size=0.1\n",
    "    )\n",
    "    \n",
    "    ## Print the shapes of the attacker training and test sets\n",
    "    print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "    \n",
    "    ## Take the first 500 samples from the attacker training set\n",
    "    attacker_X_train = attacker_X_train[0:500]\n",
    "    attacker_y_train = attacker_y_train[0:500]\n",
    "\n",
    "    ## Fit the ShadowModelBundle on the attacker training data and transform it\n",
    "    attacker_X_test =   attacker_X_test[0:60]\n",
    "    attacker_y_test =   attacker_y_test[0:60]\n",
    "    \n",
    "    \n",
    "    ## print(\"Training the shadow models...\")\n",
    "\n",
    "    \n",
    "    X_shadow, y_shadow = smb.fit_transform(\n",
    "        attacker_X_train,\n",
    "        attacker_y_train,\n",
    "        fit_kwargs=dict(\n",
    "            epochs=10,\n",
    "            verbose=False,\n",
    "            validation_data=(attacker_X_test, attacker_y_test),\n",
    "             batch_size = 10\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    ## Create an AttackModelBundle with the specified attack model function and number of classes\n",
    "    amb = AttackModelBundle(attack_model_fn_keras_dp, num_classes=2)\n",
    "\n",
    "    ## Fit the attack models using the shadow data\n",
    "    amb.fit(\n",
    "            X_shadow, y_shadow, fit_kwargs=dict(epochs=10, verbose=False)\n",
    "        )\n",
    "    \n",
    "    ## Define the size of the attack test dataset\n",
    "    ATTACK_TEST_DATASET_SIZE = 200\n",
    "    \n",
    "    ## Prepare the attack data in the expected format for the AttackModelBundle\n",
    "    data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
    "    data_out = X_test[:ATTACK_TEST_DATASET_SIZE], y_test[:ATTACK_TEST_DATASET_SIZE]\n",
    "\n",
    "    ## Compile the attack test data and real membership labels\n",
    "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "            target_model, data_in, data_out\n",
    "        )\n",
    "\n",
    "    ## Predict the membership labels using the AttackModelBundle\n",
    "    attack_guesses = amb.predict(attack_test_data)\n",
    "    \n",
    "    ## Compute the attack accuracy by comparing the predicted labels with the real membership labels\n",
    "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "    \n",
    "    \n",
    "    ## Compute privacy parameter epsilon using the compute_dp_sgd_privacy function\n",
    "    epsilon =  compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=X_train.shape[0],\n",
    "                                              batch_size=5,\n",
    "                                              noise_multiplier=noise_multiplier,\n",
    "                                              epochs=15,\n",
    "                                              delta=0.0001)\n",
    "\n",
    "    return attack_accuracy, epsilon, acc_score\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3761731",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "\n",
    "# Run the MIA attack 5 times with the selected noise multiplier\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.001,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "# Extract the epsilon values from the results\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a158c94",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.0025,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e885dfff",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f49f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.005,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "388eb723",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ecf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.006,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e2b834f",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.008,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0d81385",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(3):\n",
    "    ans =  mia_attack(0.01,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53f5610a",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32427802",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.02,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4100d5ed",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.03,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76c1428d",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.04,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e27b74fb",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abc25a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.05,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95037f82",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ef274",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.1,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eae456f3",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d067c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(0.5,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96da9678",
   "metadata": {},
   "source": [
    "### Noise Multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_acc = []\n",
    "epsilon = []\n",
    "accuracy = []\n",
    "for i in range(5):\n",
    "    ans =  mia_attack(1,1.5)\n",
    "    mia_acc.append(ans[0])\n",
    "    print('MIA Accuracy')\n",
    "    print(ans[0])\n",
    "    print()\n",
    "    print('Epsilon')\n",
    "    print(ans[1])\n",
    "    print()\n",
    "    print('Accuracy')\n",
    "    print(ans[2])\n",
    "    epsilon.append(ans[1])\n",
    "    accuracy.append(ans[2])\n",
    "\n",
    "epsilon_list = list(zip(*epsilon))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ad4fa8e",
   "metadata": {},
   "source": [
    "## Analysis with Densenet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ce4383",
   "metadata": {},
   "source": [
    "### MIA Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8180459",
   "metadata": {},
   "outputs": [],
   "source": [
    "##0.0025\n",
    "n1 = np.array([0.6175,0.5675, 0.58,0.5625, 0.595])\n",
    "\n",
    "##0.005\n",
    "n2 = np.array([0.5875, 0.565, 0.5425, 0.595, 0.5])\n",
    "\n",
    "##0.01\n",
    "n3 = np.array([0.6, 0.565, 0.585, 0.55, 0.6])\n",
    "\n",
    "##0.05\n",
    "n4 = np.array([0.3675, 0.5, 0.4625, 0.5, 0.5])\n",
    "\n",
    "\n",
    "##0.1\n",
    "n5 = np.array([0.555, 0.56, 0.5, 0.555, 0.445])\n",
    "\n",
    "\n",
    "# Calculate the average\n",
    "n1_mean = np.mean(n1)\n",
    "n2_mean = np.mean(n2)\n",
    "n3_mean = np.mean(n3)\n",
    "n4_mean = np.mean(n4)\n",
    "n5_mean = np.mean(n5)\n",
    "\n",
    "\n",
    "# Calculate the standard deviation\n",
    "n1_std = np.std(n1)\n",
    "n2_std =np.std(n2)\n",
    "n3_std =  np.std(n3)\n",
    "n4_std = np.std(n4)\n",
    "n5_std =  np.std(n5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "\n",
    "labels = [0.0025, 0.005, 0.01, 0.05, 0.1]\n",
    "labels_log = np.log([0.0025, 0.005, 0.01, 0.05, 0.1])\n",
    "x_pos = np.arange(len(labels))\n",
    "CTEs = [n1_mean, n2_mean, n3_mean, n4_mean, n5_mean]\n",
    "error = [n1_std, n2_std, n3_std, n4_std, n5_std ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffa775",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(labels_log, CTEs,\n",
    "       yerr=error,\n",
    "       align='center',\n",
    "       alpha=0,\n",
    "       ecolor='black',\n",
    "       capsize=10)\n",
    "\n",
    "ax.set_xticks(labels_log)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_title('Noise Multiplier vs MIA Accuracy')\n",
    "ax.set_xlabel('Noise Multiplier')\n",
    "ax.set_ylabel('Average MIA Accuracy')\n",
    "ax.yaxis.grid(True)\n",
    "ax.plot( labels_log, CTEs)\n",
    "ax.plot( labels_log, CTEs, 'ro')\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "# plt.savefig('bar_plot_with_error_bars.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ad0b3f4",
   "metadata": {},
   "source": [
    "### Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##0.0025\n",
    "n1 = np.array( [0.8668,0.8635,0.8539, 0.900481, 0.9325])\n",
    "\n",
    "##0.005\n",
    "n2 = np.array( [0.84109, 0.865168539, 0.91011236, 0.91011236, 0.37720706260032105])\n",
    "\n",
    "##0.01\n",
    "n3 = np.array( [0.825, 0.841, 0.801, 0.9117, 0.7223])\n",
    "\n",
    "##0.05\n",
    "n4 = np.array([0.7479935794542536, 0.43820224719101125, 0.8218298555377207, 0.6260032102728732, 0.37720706260032105])\n",
    "\n",
    "\n",
    "##0.1\n",
    "n5 = np.array([0.6260032102728732,0.6179775280898876,0.6260032102728732,0.6260032102728732,0.6260032102728732])\n",
    "\n",
    "\n",
    "# Calculate the average\n",
    "n1_mean = np.mean(n1)\n",
    "n2_mean = np.mean(n2)\n",
    "n3_mean = np.mean(n3)\n",
    "n4_mean = np.mean(n4)\n",
    "n5_mean = np.mean(n5)\n",
    "\n",
    "\n",
    "# Calculate the standard deviation\n",
    "n1_std = np.std(n1)\n",
    "n2_std =np.std(n2)\n",
    "n3_std =  np.std(n3)\n",
    "n4_std = np.std(n4)\n",
    "n5_std =  np.std(n5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "\n",
    "labels = [0.0025, 0.005, 0.01, 0.05, 0.1]\n",
    "labels_log = np.log([0.0025, 0.005, 0.01, 0.05, 0.1])\n",
    "x_pos = np.arange(len(labels))\n",
    "CTEs = [n1_mean, n2_mean, n3_mean, n4_mean, n5_mean]\n",
    "error = [n1_std, n2_std, n3_std, n4_std, n5_std ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594eb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(labels_log, CTEs,\n",
    "       yerr=error,\n",
    "       align='center',\n",
    "       alpha=0,\n",
    "       ecolor='black',\n",
    "       capsize=10)\n",
    "\n",
    "ax.set_xticks(labels_log)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_title('Noise Multiplier vs MIA Accuracy')\n",
    "ax.set_xlabel('Noise Multiplier')\n",
    "ax.set_ylabel('Average MIA Accuracy')\n",
    "ax.yaxis.grid(True)\n",
    "ax.plot( labels_log, CTEs)\n",
    "ax.plot( labels_log, CTEs, 'ro')\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "# plt.savefig('bar_plot_with_error_bars.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_multiplier = [0.0010, 0.0025,0.0050, 0.0060,0.0080, 0.0100, 0.0200, 0.0300, 0.0400,0.0500]  \n",
    "eps = []\n",
    "for i in range(len(noise_multiplier)):\n",
    "    \n",
    "    epsilon =  compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=X_train.shape[0],\n",
    "                                                  batch_size=10,\n",
    "                                                  noise_multiplier=noise_multiplier[i],\n",
    "                                                  epochs=25,\n",
    "                                                  delta=0.0001)\n",
    "    eps.append(epsilon[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b557a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
